 

## 2.3  Models

Model 是 Agent 的大脑，负责处理所有输入和输出数据。通过有效调用不同的模型，智能体可以根据任务需求执行文本分析、图像识别和复杂推理等操作。CAMEL 提供了一系列标准和可定制的接口，并与各种组件无缝集成，以赋能大语言模型（LLM）驱动的应用程序开发。在本部分，我们将介绍 CAMEL 目前支持的模型、工作原理及与模型交互的方式。

### 2.3.1 目前支持的模型

[可以点击此处查看目前支持的模型](https://fmhw1n4zpn.feishu.cn/docx/AF4XdOZpIo6TOaxzDK8cxInNnCe#share-SuUOdEh28or0FSxKovmcVbLsnGh)

### 2.3.2  通过API调用模型

我们可以通过使用ModelFactory的create方法创建不同的model，然后修改以下三个参数就可以做到调用不同的模型：`model_platform、model_type、model_config_dict`

示例API申请地址

[智谱AI开放平台](https://open.bigmodel.cn/launch?spreadparam=datawhale\&utm_source=datawhale\&utm_campaign=%E6%88%98%E7%95%A5%E7%94%9F%E6%80%81%E9%83%A8-%E6%B8%A0%E9%81%93%E5%90%88%E4%BD%9C&_channel_track_key=GRfyviFi)

```python
import os

from camel.agents import ChatAgent
from camel.configs import ZhipuAIConfig
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.ZHIPU,
    model_type=ModelType.GLM_4,
    model_config_dict=ZhipuAIConfig(temperature=0.2).as_dict(),
    api_key=os.environ.get("ZHIPUAI_API_KEY"),
    url=os.environ.get("ZHIPUAI_API_BASE_URL"),
)

# 设置system prompt
sys_msg = BaseMessage.make_assistant_message(
    role_name="Assistant",
    content="You are a helpful assistant.",
)

# 初始化agent
camel_agent = ChatAgent(system_message=sys_msg, model=model, output_language="zh")#这里同样可以设置输出语言

user_msg = BaseMessage.make_user_message(
    role_name="User",
    content="""Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents.""",
)


# 调用模型
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

#以下是模型回复的内容
'''
===============================================================================
你好，向CAMEL AI这个致力于自主交互式智能体研究的开源社区问好。
===============================================================================
'''
```

如果您想使用与 OpenAI 的 API 兼容的接口（即遵循 OpenAI 提供的 API 规范和认证方式），可以将model替换为以下代码：`model`

```python
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="a-string-representing-the-model-type",
    api_key=os.environ.get("OPENAI_COMPATIBILIY_API_KEY"),
    url=os.environ.get("OPENAI_COMPATIBILIY_API_BASE_URL"),
    model_config_dict={"temperature": 0.4, "max_tokens": 4096},
)
```

### 2.3.3 使用开源模型



**使用Ollama**

Ollama 是一个开源的机器学习框架，专注于让用户轻松地创建和使用各种语言模型。它提供了一个简单的 API，能够将预训练的语言模型（例如 GPT 系列）集成到你的应用程序中。Ollama 支持许多主流的机器学习模型和任务，如文本生成、对话系统、文本分类等。通过它的简单接口，开发者能够方便地进行模型加载、推理以及与模型交互。



进阶方案是在后端部署一个带有本地模型的服务器，并将其用作 API 的本地替代品。我们在这里使用 Ollama 部署的 Qwen2.5 为例。

0. 首先安装[Ollama](https://ollama.com/download)

2) 设置 Ollama 后，通过在终端中键入以下命令来拉取 Qwen2.5模型（这里使用7B的模型，大约需要16GB的内存，70B的模型大约需要64GB以上的内存，可以根据自身的配置和需要选择不同参数的模型），Ollama基于llama.cpp实现，本地CPU推理效率非常高（当然如果有GPU的话，推理效率会更高）, 还可以兼容 openai的接口。 ：

```bash
ollama pull qwen2.5:7b
```

* 在项目目录中创建一个类似于下面的 ModelFile（可选）。

```plain&#x20;text
FROM qwen2.5:7b

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
"""
```

* 接下来，创建模型（可选）：

```bash
ollama create camel-qwen2.5 -f ./Modelfile
```

之后同样使用以下代码替换model：`model`

```python
#如果出现openai.InternalServerError: Error code: 502错误可以尝试设置一下http_proxy及https_proxy
#import os

# os.environ["http_proxy"] = "http://localhost:11434/v1/"
# os.environ["https_proxy"] = "http://localhost:11434/v1/"

model = ModelFactory.create(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="qwen2.5",
    model_config_dict={"temperature": 0.4},#可选
)
```

**使用vLLM**

vLLM 是一个高效的、面向大规模语言模型的推理库，专为大规模语言模型设计，旨在提高推理速度和减少资源消耗。vLLM 的一个核心特性是它能够在多种硬件环境下（例如单 GPU、多 GPU，甚至 CPU）高效运行，极大地降低了推理成本。



1. 首先安装[vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html)

2. 设置 vLLM 后，启动兼容 OpenAI 的服务器，例如：

```python
python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-mini-4k-instruct --api-key vllm --dtype bfloat16
```

之后同样使用以下代码替换model：`model`

```python
vllm_model = ModelFactory.create(
    model_platform=ModelPlatformType.VLLM,
    model_type="microsoft/Phi-3-mini-4k-instruct",
    url="http://localhost:8000/v1", # 可选
    model_config_dict={"temperature": 0.4}, #可选
    )
```

